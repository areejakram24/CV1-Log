
# Performance Metrics 

This is carried out on coco dataset 

##Performant Metric Plots: 
1. results:
/Users/cheesecake/projai/runs/detect/train/results.png

The first row shows how well the model fits the training data (Trainig Loss). 
The bottom row shows Validation Loss and mAP (how well the model generalizes to new data).
 If validation loss increases and training loss decreases, the model seems to be overfitting.


2. BoxPR_curve:
/Users/cheesecake/projai/runs/detect/train/BoxPR_curve.png

It is a plot of Precision vs. Recall. 
The Area Under the Curve gives you the mAP.

3. BoxF1_curve
/Users/cheesecake/projai/runs/detect/train/BoxF1_curve.png

This plot shows the F1 score over different confidence values. The peak F1 value 
(0.61 in the graph) is the optimal confidence value for this model.


4. Confusion Matrix
/Users/cheesecake/projai/runs/detect/train/confusion_matrix.png

/Users/cheesecake/projai/runs/detect/train/confusion_matrix_normalized.png

The confusion matrix shows where the model is making mistakes. For each class, it shows which other classes loosk similar to one class.
The normalized confusion matrix shows high confidence in predicting the 'person' class, but also a slight confusion between 'dog' and 'horse'.

Note:
Validation loss started to increase after epoch 70, and training loss decreased continually, meaning, the model began to overfit.

## Performance Metric Plots: Validation Results

1. Ground Truth:

/Users/cheesecake/projai/runs/detect/val/val_batch0_labels.jpg

This image shows a batch of validation images overlaid with the true bounding boxes and labels. In the image, the objects like 'dog', 'horse', 'elephant', 'person', and 'potted plant' are correctly labeled according to the ground truth.


2. Model Predictions:
/Users/cheesecake/projai/runs/detect/val/val_batch0_pred.jpg

This image contains the bounding boxes and predicted labels generated by the pre-trained YOLO model for the same set of validation images.
In the top-left image, the model correctly identifies the 'dog' but also mistakes a 'suitcase' on the left side, indicating a **False Positive**. The bounding box around the person in the top-right image is divided into two more boxes with low confidence,probably showing classification uncertainty.

3. Precision-Recall Plot

/Users/cheesecake/projai/runs/detect/val/BoxPR_curve.png
The plot shows that the Mean Average Precision (mAP), the most important metric.
 A mAP = 0.698 means that at an Intersection-over-Union (IoU) threshold of 0.5, the average precision across all classes is nearly 70%.
 The 'person' class, has the lowest average precision of just 0.578. SO it can be improved.
 
 4. F1 confidence curve
 /Users/cheesecake/projai/runs/detect/val/BoxF1_curve.png
 
 The optimal confidence threshold for running inference is 0.282. At this threshold, we can achieve the best balance between Precision and Recall (since the F1 Score is 0.67).
 
 5. Precision-Confidenc Curve
/Users/cheesecake/projai/runs/detect/val/BoxP_curve.png
We can see form the plot that the mdoel achieves a high precision , of 0.83, on a higher thereshold.


6. Recall curve:
/Users/cheesecake/projai/runs/detect/val/BoxPR_curve.png

The highest Recall of 0.65 is achieved at the lowest possible threshold(0.0).
We may say that the model is still missing about 35% of objects in the validation set, even when allowing for very low-confidence predictions.

7. Confusion matrices:
This shows the types of errors model is making.
The main diagonal shows correct predictions. The off-diagonal shows confusions.
Since the matrix is mostly empty except for the top-left corner, it indicates the model is highly focused on the first few classes and rarely confuses them with other objects, but the overall number of detections is quite low.
